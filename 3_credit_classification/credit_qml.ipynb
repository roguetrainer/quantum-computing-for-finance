{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Credit Classification with Quantum Machine Learning\n",
    "\n",
    "This notebook demonstrates variational quantum classifiers (VQC) and quantum kernel methods for credit risk classification.\n",
    "\n",
    "**Goal:** Classify loan applicants as creditworthy or risky using quantum ML\n",
    "\n",
    "**Author:** Ian Buckley  \n",
    "**Date:** 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "n_samples = 200\n",
    "n_features = 4\n",
    "test_size = 0.3\n",
    "random_state = 42\n",
    "\n",
    "# Quantum parameters\n",
    "n_qubits = n_features\n",
    "n_layers = 2\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 50\n",
    "batch_size = 25\n",
    "learning_rate = 0.01\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"VQC: {n_qubits} qubits, {n_layers} layers\")\n",
    "print(f\"Training: {n_epochs} epochs, batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Generate Credit Data\n",
    "\n",
    "Features represent:\n",
    "- Income level\n",
    "- Debt-to-income ratio\n",
    "- Credit history length\n",
    "- Employment years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_credit_data(n_samples, n_features, random_state=42):\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_features,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=1,\n",
    "        flip_y=0.1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    y_quantum = 2 * y - 1  # Convert to -1, +1\n",
    "    feature_names = ['income', 'debt_ratio', 'credit_history', 'employment_years']\n",
    "    return X, y, y_quantum, feature_names\n",
    "\n",
    "X, y, y_quantum, feature_names = generate_credit_data(n_samples, n_features, random_state)\n",
    "\n",
    "# Split and normalize\n",
    "X_train, X_test, y_train, y_test, y_train_q, y_test_q = train_test_split(\n",
    "    X, y, y_quantum, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Scale to [0, π] for quantum encoding\n",
    "X_train_scaled = np.pi * (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "X_test_scaled = np.pi * (X_test - X_test.min()) / (X_test.max() - X_test.min())\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Class balance: {np.sum(y_train==1)}/{np.sum(y_train==0)} (creditworthy/risky)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA projection for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                     c=y_train, cmap='RdYlGn', alpha=0.6, s=50)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('Training Data (PCA Projection)')\n",
    "plt.colorbar(scatter, label='Class', ticks=[0, 1])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(n_features), pca.components_[0], alpha=0.7, label='PC1')\n",
    "plt.bar(range(n_features), pca.components_[1], alpha=0.7, label='PC2')\n",
    "plt.xticks(range(n_features), feature_names, rotation=45, ha='right')\n",
    "plt.ylabel('Component Loading')\n",
    "plt.title('Principal Component Loadings')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance explained: {sum(pca.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Classical SVM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Classical SVM...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=random_state)\n",
    "svm.fit(X_train, y_train)\n",
    "classical_time = time.time() - start_time\n",
    "\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "print(f\"Training time: {classical_time:.2f}s\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                           target_names=['Risky', 'Creditworthy']))\n",
    "\n",
    "cm_classical = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_classical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Quantum Feature Map\n",
    "\n",
    "Encodes classical data into quantum states with entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_feature_map(x, wires):\n",
    "    \"\"\"ZZ feature map with entanglement.\"\"\"\n",
    "    n = len(wires)\n",
    "    \n",
    "    # First layer\n",
    "    for i in wires:\n",
    "        qml.Hadamard(wires=i)\n",
    "        qml.RZ(x[i], wires=i)\n",
    "    \n",
    "    # Entangling layer\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            qml.CNOT(wires=[wires[i], wires[j]])\n",
    "            qml.RZ(x[i] * x[j], wires=wires[j])\n",
    "            qml.CNOT(wires=[wires[i], wires[j]])\n",
    "\n",
    "print(\"✓ Quantum feature map defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Variational Quantum Classifier\n",
    "\n",
    "Architecture:\n",
    "1. Feature map (encodes data)\n",
    "2. Variational layers (trainable)\n",
    "3. Measurement (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_layer(params, wires):\n",
    "    \"\"\"Single layer of variational circuit.\"\"\"\n",
    "    n = len(wires)\n",
    "    \n",
    "    for i, wire in enumerate(wires):\n",
    "        qml.RY(params[i, 0], wires=wire)\n",
    "        qml.RZ(params[i, 1], wires=wire)\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "    qml.CNOT(wires=[wires[n-1], wires[0]])\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def vqc_circuit(x, params):\n",
    "    \"\"\"Variational quantum classifier circuit.\"\"\"\n",
    "    quantum_feature_map(x, wires=range(n_qubits))\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        variational_layer(params[layer], wires=range(n_qubits))\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "print(f\"✓ VQC circuit defined\")\n",
    "print(f\"  Qubits: {n_qubits}\")\n",
    "print(f\"  Layers: {n_layers}\")\n",
    "print(f\"  Parameters: {n_layers * n_qubits * 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Train VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "np.random.seed(random_state)\n",
    "params = np.random.uniform(0, 2*np.pi, (n_layers, n_qubits, 2), requires_grad=True)\n",
    "\n",
    "optimizer = AdamOptimizer(learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"\\nTraining VQC...\\n\")\n",
    "print(\"Epoch  Train Loss  Train Acc  Test Acc   Time\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(X_train_scaled))\n",
    "    X_train_shuffled = X_train_scaled[indices]\n",
    "    y_train_shuffled = y_train_q[indices]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(X_train_scaled), batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        \n",
    "        def cost_fn(params):\n",
    "            predictions = np.array([vqc_circuit(x, params) for x in X_batch])\n",
    "            loss = np.mean((y_batch - predictions)**2)\n",
    "            return loss\n",
    "        \n",
    "        params, batch_loss = optimizer.step_and_cost(cost_fn, params)\n",
    "        epoch_loss += batch_loss\n",
    "    \n",
    "    epoch_loss /= (len(X_train_scaled) // batch_size)\n",
    "    \n",
    "    # Evaluate\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        train_predictions = np.array([vqc_circuit(x, params) for x in X_train_scaled])\n",
    "        train_predictions_binary = np.sign(train_predictions)\n",
    "        train_acc = accuracy_score(y_train_q, train_predictions_binary)\n",
    "        \n",
    "        test_predictions = np.array([vqc_circuit(x, params) for x in X_test_scaled])\n",
    "        test_predictions_binary = np.sign(test_predictions)\n",
    "        test_acc = accuracy_score(y_test_q, test_predictions_binary)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f\"{epoch+1:4d}   {epoch_loss:.4f}      {train_acc:.3f}      {test_acc:.3f}     {epoch_time:.1f}s\")\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "vqc_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training complete in {vqc_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Evaluate VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.array([vqc_circuit(x, params) for x in X_test_scaled])\n",
    "test_predictions_binary = np.sign(test_predictions)\n",
    "vqc_test_acc = accuracy_score(y_test_q, test_predictions_binary)\n",
    "\n",
    "y_test_01 = (y_test_q + 1) // 2\n",
    "test_pred_01 = (test_predictions_binary + 1) // 2\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {vqc_test_acc:.3f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_01, test_pred_01,\n",
    "                           target_names=['Risky', 'Creditworthy']))\n",
    "\n",
    "cm_vqc = confusion_matrix(y_test_01, test_pred_01)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_vqc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Quantum Kernel SVM\n",
    "\n",
    "Alternative approach: Use quantum feature map to define kernel, then use classical SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_kernel(x1, x2, n_qubits):\n",
    "    \"\"\"Compute quantum kernel between two data points.\"\"\"\n",
    "    dev_kernel = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev_kernel)\n",
    "    def kernel_circuit(x1, x2):\n",
    "        quantum_feature_map(x1, wires=range(n_qubits))\n",
    "        qml.adjoint(quantum_feature_map)(x2, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "    \n",
    "    probs = kernel_circuit(x1, x2)\n",
    "    return probs[0]\n",
    "\n",
    "def compute_kernel_matrix(X1, X2, n_qubits, desc=\"\"):\n",
    "    \"\"\"Compute kernel matrix.\"\"\"\n",
    "    n1, n2 = len(X1), len(X2)\n",
    "    K = np.zeros((n1, n2))\n",
    "    \n",
    "    print(f\"Computing {desc} kernel matrix ({n1}×{n2})...\")\n",
    "    for i in range(n1):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Progress: {i*n2}/{n1*n2} ({100*i/n1:.0f}%)\")\n",
    "        for j in range(n2):\n",
    "            K[i, j] = quantum_kernel(X1[i], X2[j], n_qubits)\n",
    "    print(f\"  Complete: {n1*n2}/{n1*n2} (100%)\\n\")\n",
    "    \n",
    "    return K\n",
    "\n",
    "print(\"\\nQuantum Kernel SVM\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute kernels\n",
    "K_train = compute_kernel_matrix(X_train_scaled, X_train_scaled, n_qubits, \"training\")\n",
    "K_test = compute_kernel_matrix(X_test_scaled, X_train_scaled, n_qubits, \"test\")\n",
    "\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Train SVM\n",
    "svm_kernel = SVC(kernel='precomputed')\n",
    "y_train_01 = (y_train_q + 1) // 2\n",
    "svm_kernel.fit(K_train, y_train_01)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_kernel = svm_kernel.predict(K_test)\n",
    "kernel_test_acc = accuracy_score(y_test_01, y_test_pred_kernel)\n",
    "\n",
    "print(f\"Test Accuracy: {kernel_test_acc:.3f}\")\n",
    "print(f\"Total time: {kernel_time:.1f}s\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_01, y_test_pred_kernel,\n",
    "                           target_names=['Risky', 'Creditworthy']))\n",
    "\n",
    "cm_kernel = confusion_matrix(y_test_01, y_test_pred_kernel)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"{'Method':<20} {'Accuracy':<12} {'Training Time':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Classical SVM':<20} {test_acc:<12.3f} {classical_time:<15.2f}s\")\n",
    "print(f\"{'Quantum VQC':<20} {vqc_test_acc:<12.3f} {vqc_time:<15.2f}s\")\n",
    "print(f\"{'Quantum Kernel SVM':<20} {kernel_test_acc:<12.3f} {kernel_time:<15.2f}s\")\n",
    "print()\n",
    "print(\"Notes:\")\n",
    "print(\"  • Quantum VQC: Competitive accuracy, much slower training\")\n",
    "print(\"  • Quantum Kernel: Similar accuracy, slow kernel computation\")\n",
    "print(\"  • Classical: Fast and competitive\")\n",
    "print(\"  • Small dataset - quantum advantage not expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. Training Loss\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "epochs = np.arange(1, len(train_losses)+1) * 10\n",
    "ax1.plot(epochs, train_losses, 'o-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('VQC Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy Curves\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.plot(epochs, train_accs, 'o-', label='Train', linewidth=2)\n",
    "ax2.plot(epochs, test_accs, 's-', label='Test', linewidth=2)\n",
    "ax2.axhline(y=test_acc, color='red', linestyle='--', label='Classical SVM', alpha=0.7)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('VQC Training Progress')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Method Comparison\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "methods = ['Classical\\nSVM', 'Quantum\\nVQC', 'Quantum\\nKernel']\n",
    "accuracies = [test_acc, vqc_test_acc, kernel_test_acc]\n",
    "colors = ['red', 'green', 'blue']\n",
    "bars = ax3.bar(methods, accuracies, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Test Accuracy')\n",
    "ax3.set_title('Accuracy Comparison')\n",
    "ax3.set_ylim([0, 1])\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Training Time\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "times = [classical_time, vqc_time, kernel_time]\n",
    "bars = ax4.bar(methods, times, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('Time (seconds)')\n",
    "ax4.set_title('Training Time Comparison')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# 5. Feature Space\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "y_plot = (y_train_q + 1) // 2\n",
    "scatter = ax5.scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                     c=y_plot, cmap='RdYlGn', alpha=0.6, s=50)\n",
    "ax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax5.set_title('Feature Space (PCA Projection)')\n",
    "plt.colorbar(scatter, ax=ax5, label='Class', ticks=[0, 1])\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Quantum Kernel Matrix\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "im = ax6.imshow(K_train, cmap='viridis', aspect='auto')\n",
    "ax6.set_xlabel('Training Sample')\n",
    "ax6.set_ylabel('Training Sample')\n",
    "ax6.set_title('Quantum Kernel Matrix')\n",
    "plt.colorbar(im, ax=ax6, label='Kernel Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('credit_classification_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "✅ Quantum ML methods competitive but not superior for small data  \n",
    "✅ VQC shows promise with proper hyperparameter tuning  \n",
    "✅ Quantum kernels provide alternative approach to feature mapping  \n",
    "\n",
    "### Challenges\n",
    "❌ Training time significantly higher than classical  \n",
    "❌ Barren plateau problem in deep circuits  \n",
    "❌ Quantum advantage likely only for larger, structured datasets  \n",
    "\n",
    "### When to Use Quantum ML\n",
    "- High-dimensional data (>10 features)\n",
    "- Complex feature interactions\n",
    "- When quantum kernels express patterns classical kernels miss\n",
    "- Research and algorithm development\n",
    "\n",
    "### Next Steps\n",
    "- Try more features (6-8)\n",
    "- Experiment with different feature maps\n",
    "- Test on imbalanced data\n",
    "- Implement cross-validation\n",
    "- Try on real credit data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
